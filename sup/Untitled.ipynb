{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentiment import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data\n",
      "-- train data\n",
      "sentiment/train.tsv\n",
      "4582\n",
      "-- dev data\n",
      "sentiment/dev.tsv\n",
      "458\n",
      "-- transforming data and labels\n",
      "\n",
      "Training classifier\n",
      "\n",
      "Evaluating\n",
      "  Accuracy on train  is: 1.0\n",
      "  Accuracy on dev  is: 0.8296943231441049\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading data\")\n",
    "tarfname = \"data/sentiment.tar.gz\"\n",
    "sentiment = read_files(tarfname)\n",
    "print(\"\\nTraining classifier\")\n",
    "import classify\n",
    "cls = classify.train_classifier(sentiment.trainX, sentiment.trainy)\n",
    "print(\"\\nEvaluating\")\n",
    "classify.evaluate(sentiment.trainX, sentiment.trainy, cls, 'train')\n",
    "classify.evaluate(sentiment.devX, sentiment.devy, cls, 'dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['can']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"can\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sentiment.read_files.<locals>.LemmaTokenizer at 0x110778f98>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment.count_vect.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "yp = cls.predict(sentiment.devX)\n",
    "print(sum(yp!=sentiment.devy))\n",
    "falseIndex = np.where(yp!=sentiment.devy)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5,   7,  11,  12,  15,  25,  29,  34,  42,  45,  46,  48,  49,\n",
       "        58,  68,  76,  82,  87,  88,  96,  98, 114, 117, 124, 127, 130,\n",
       "       145, 147, 162, 163, 166, 175, 179, 190, 200, 206, 207, 218, 223,\n",
       "       236, 247, 248, 252, 254, 255, 258, 260, 267, 268, 283, 289, 291,\n",
       "       300, 301, 303, 311, 312, 333, 336, 338, 339, 359, 364, 366, 379,\n",
       "       393, 396, 397, 398, 399, 406, 412, 417, 424, 433, 436, 442, 456])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "falseIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5,   7,  11,  12,  15,  25,  29,  34,  42,  45,  46,  48,  49,\n",
       "        58,  68,  76,  82,  87,  88,  96,  98, 114, 117, 124, 127, 130,\n",
       "       145, 147, 162, 163, 166, 175, 179, 190, 200, 206, 207, 218, 223,\n",
       "       236, 247, 248, 252, 254, 255, 258, 260, 267, 268, 283, 289, 291,\n",
       "       300, 301, 303, 309, 311, 312, 333, 336, 338, 339, 359, 364, 366,\n",
       "       379, 393, 396, 397, 398, 399, 406, 412, 417, 424, 433, 436, 442,\n",
       "       456])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "falseIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Normally LOVE Penn Station, but today was. Greeted by a gnat infestation @ my Penn Station. Sooo troubled by that! Please get rid of the gnats. I will be back'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment.dev_data[309]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', \"'m\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "word_tokenize(\"I'm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', \"'m\", 'are', \"n't\"]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"I 'm are n't\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've been looking forward to trying Khotan ever since social house closed and I finally got to check it out.This review isn't focused on the food as much as it\n",
      "---------\n",
      "trying / to check it / to check / to / the food a / the food / the / social / since / review is n't / review is / review / on the food / on the / on / n't / much a / much / looking forward to / looking forward / looking / it / is n't / is / i finally got / i finally / i 've been / i 've / i / house / got to / got / forward to / forward / food a / food / focused / finally got / finally / ever since / ever / closed and / closed / check it / check / been looking forward / been looking / been / and i finally / and i / and / a much a / a much / a it / a / 've been looking / 've been / 've\n",
      "---------\n",
      "POSITIVE\n",
      "0\n",
      "58\n",
      "['I', 'Khotan', 'I', 'out.This', 'as', 'as']\n",
      "['I', \"'ve\", 'been', 'looking', 'forward', 'to', 'trying', 'Khotan', 'ever', 'since', 'social', 'house', 'closed', 'and', 'I', 'finally', 'got', 'to', 'check', 'it', 'out.This', 'review', 'is', \"n't\", 'focused', 'on', 'the', 'food', 'as', 'much', 'as', 'it']\n"
     ]
    }
   ],
   "source": [
    "ind_ = 31\n",
    "print(sentiment.dev_data[falseIndex[ind_]], end='\\n---------\\n')\n",
    "tokens = sentiment.count_vect.inverse_transform(sentiment.devX[falseIndex[ind_]])[0]\n",
    "print(' / '.join(tokens), end='\\n---------\\n')\n",
    "print(sentiment.dev_labels[falseIndex[ind_]])\n",
    "print(yp[falseIndex[ind_]])\n",
    "print(len(tokens))\n",
    "# print(set([s.lower() for s in sentiment.dev_data[falseIndex[ind_]].split()]).difference(set(tokens)))\n",
    "print([w for w in word_tokenize(sentiment.dev_data[falseIndex[ind_]]) if w not in tokens])\n",
    "print(word_tokenize(sentiment.dev_data[falseIndex[ind_]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'m\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.sub(r'^[_-]*','',\"'m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benelux on Wellington frustrated me beyond words last night.After finishing a Zumba class across the street, a friend and I popped in here for  a quick drink and something\n",
      "---------\n",
      "word / wellington / the street , / the street / the / street , / street / something / quick drink / quick / popped in here / popped in / popped / on / me / last / in here for / in here / in / i popped / i / here for a / here for / here / friend and i / friend and / friend / for a quick / for a / for / finishing / drink and / drink / class / beyond / and something / and i popped / and i / and / across the street / across the / across / a quick drink / a quick / a friend and / a friend / a / , a / ,\n",
      "---------\n",
      "NEGATIVE\n",
      "1\n",
      "49\n",
      "['Benelux', 'Wellington', 'frustrated', 'words', 'night.After', 'Zumba', 'I']\n",
      "['Benelux', 'on', 'Wellington', 'frustrated', 'me', 'beyond', 'words', 'last', 'night.After', 'finishing', 'a', 'Zumba', 'class', 'across', 'the', 'street', ',', 'a', 'friend', 'and', 'I', 'popped', 'in', 'here', 'for', 'a', 'quick', 'drink', 'and', 'something']\n"
     ]
    }
   ],
   "source": [
    "ind_ = 22\n",
    "print(sentiment.dev_data[falseIndex[ind_]], end='\\n---------\\n')\n",
    "tokens = sentiment.count_vect.inverse_transform(sentiment.devX[falseIndex[ind_]])[0]\n",
    "print(' / '.join(tokens), end='\\n---------\\n')\n",
    "print(sentiment.dev_labels[falseIndex[ind_]])\n",
    "print(yp[falseIndex[ind_]])\n",
    "print(len(tokens))\n",
    "# print(set([s.lower() for s in sentiment.dev_data[falseIndex[ind_]].split()]).difference(set(tokens)))\n",
    "print([w for w in word_tokenize(sentiment.dev_data[falseIndex[ind_]]) if w not in tokens])\n",
    "print(word_tokenize(sentiment.dev_data[falseIndex[ind_]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some words in test set not contained in vocabulary built by training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'relief' in sentiment.count_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment.count_vect.stop_words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'had': 10,\n",
       " 'an': 4,\n",
       " 'awsome': 6,\n",
       " 'late': 14,\n",
       " 'lunch': 15,\n",
       " 'at': 5,\n",
       " 'this': 22,\n",
       " 'bbq': 7,\n",
       " 'restaurant': 18,\n",
       " 'afternoon': 2,\n",
       " 'i': 12,\n",
       " 'have': 11,\n",
       " 'a': 0,\n",
       " 'gluten': 9,\n",
       " 'alergy': 3,\n",
       " 'so': 19,\n",
       " 'it': 13,\n",
       " 'was': 24,\n",
       " 'relief': 17,\n",
       " 'to': 23,\n",
       " 'be': 8,\n",
       " 'able': 1,\n",
       " 'talk': 20,\n",
       " 'the': 21,\n",
       " 'owner': 16}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vect = TfidfVectorizer(ngram_range=(1,1),\n",
    "                       norm=None,\n",
    "                       token_pattern=r\"(?u)\\b\\w+[\\'\\!\\*]*\\w*\\b\",\n",
    "                       sublinear_tf=True,\n",
    "                       stop_words=[])\n",
    "vect.fit_transform([sentiment.dev_data[falseIndex[ind_]]])\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizer test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer  \n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Benelux', 'NNP'),\n",
       " ('on', 'IN'),\n",
       " ('Wellington', 'NNP'),\n",
       " ('frustrated', 'VBD'),\n",
       " ('me', 'PRP'),\n",
       " ('beyond', 'IN'),\n",
       " ('words', 'NNS'),\n",
       " ('last', 'JJ'),\n",
       " ('night.After', 'JJ'),\n",
       " ('finishing', 'VBG'),\n",
       " ('a', 'DT'),\n",
       " ('Zumba', 'NNP'),\n",
       " ('class', 'NN'),\n",
       " ('across', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('street', 'NN'),\n",
       " (',', ','),\n",
       " ('a', 'DT'),\n",
       " ('friend', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('I', 'PRP'),\n",
       " ('popped', 'VBD'),\n",
       " ('in', 'IN'),\n",
       " ('here', 'RB'),\n",
       " ('for', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('quick', 'JJ'),\n",
       " ('drink', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('something', 'NN')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(word_tokenize(\"I ate an apple\"))\n",
    "pos_tag(word_tokenize(\"Benelux on Wellington frustrated me beyond words last night.After finishing a Zumba class across the street, a friend and I popped in here for  a quick drink and something\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['After',\n",
       " 'finish',\n",
       " 'a',\n",
       " 'Zumba',\n",
       " 'class',\n",
       " 'across',\n",
       " 'the',\n",
       " 'street',\n",
       " ',',\n",
       " 'a',\n",
       " 'friend',\n",
       " 'and',\n",
       " 'I',\n",
       " 'pop',\n",
       " 'in',\n",
       " 'here',\n",
       " 'for',\n",
       " 'quick',\n",
       " 'drink',\n",
       " 'and',\n",
       " 'something']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.lemma = WordNetLemmatizer()\n",
    "        # self.tokenizer = RegexpTokenizer(r\"(?u)\\b\\w+[\\'\\!\\*]*\\w*\\b\")\n",
    "        # self.wnl = PorterStemmer()\n",
    "\n",
    "    def lemmatize(self, token, tag):  \n",
    "        if tag[0].lower() in ['n','v']:  \n",
    "            return self.lemma.lemmatize(token, tag[0].lower())  \n",
    "        return token\n",
    "\n",
    "    def __call__(self, sentence):\n",
    "        # return [self.lemma.lemmatize(t, 'n') for t in word_tokenize(sentence)]   \n",
    "        return [self.lemmatize(t, tag) for t, tag in pos_tag(word_tokenize(sentence))]  \n",
    "\n",
    "lemmaTokenizer = LemmaTokenizer()\n",
    "lemmaTokenizer(\"After finishing a Zumba class across the street, a friend and I popped in here for quick drinks and something\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f***k\n",
      "amazzing\n",
      "i'm\n",
      "aren't\n",
      "'t\n",
      "-not\n",
      "popped\n",
      "eat\n",
      "pop\n",
      "rock\n",
      "corpus\n",
      "interestingly\n",
      "pop\n"
     ]
    }
   ],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "print(wnl.lemmatize('f***k'))\n",
    "print(wnl.lemmatize('amazzing'))\n",
    "print(wnl.lemmatize(\"i'm\"))\n",
    "print(wnl.lemmatize(\"aren't\"))\n",
    "print(wnl.lemmatize(\"'t\"))\n",
    "print(wnl.lemmatize(\"-not\"))\n",
    "print(wnl.lemmatize(\"popped\"))\n",
    "print(wnl.lemmatize(\"ate\", 'v'))\n",
    "print(wnl.lemmatize(\"popped\", 'v'))\n",
    "print(wnl.lemmatize(\"rocks\"))\n",
    "print(wnl.lemmatize(\"corpora\"))\n",
    "print(wnl.lemmatize(\"interestingly\", wordnet.ADJ))\n",
    "print(wnl.lemmatize(\"pops\", 'n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('generalization'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word_tokensize tokenizes 'aren't' as 'are' and 'n't', that's awesome\n",
    "### word_tokenize also keeps all punctations\n",
    "### word_tokenize behaves better than my own regualr expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " \"'m\",\n",
       " 'stu',\n",
       " '!',\n",
       " 'd',\n",
       " '.',\n",
       " 'Fu**k',\n",
       " 'u.',\n",
       " 'are',\n",
       " \"n't\",\n",
       " 'id*T.',\n",
       " 'have',\n",
       " \"n't\",\n",
       " 'done',\n",
       " 'yet',\n",
       " '!',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'an',\n",
       " 'awsome',\n",
       " 'word',\n",
       " '.',\n",
       " '--',\n",
       " '-not',\n",
       " 't_is',\n",
       " '_is',\n",
       " 'true',\n",
       " ',',\n",
       " 'right',\n",
       " 'a',\n",
       " ',']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"I'm stu!d. Fu**k u. aren't id*T. haven't done yet! it's  an awsome word. ---not t_is _is true, right a,\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "strip leading and tailing '-' '_' '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spell correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from symspellpy.symspellpy import SymSpell, Verbosity\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_edit_distance_dictionary = 2\n",
    "prefix_length = 7\n",
    "# create object\n",
    "sym_spell = SymSpell(max_edit_distance_dictionary, prefix_length)\n",
    "# load dictionary\n",
    "dictionary_path = os.path.join(os.path.dirname('.'),\n",
    "                               \"frequency_dictionary_en_82_765.txt\")\n",
    "term_index = 0  # column of the term in the dictionary text file\n",
    "count_index = 1  # column of the term frequency in the dictionary text file\n",
    "if not sym_spell.load_dictionary(dictionary_path, term_index, count_index):\n",
    "    raise(\"Dictionary file not found\")\n",
    "    \n",
    "# input_term = (\"whereis th elove hehad dated forImuch of thepast who \"\n",
    "#               \"couqdn'tread in sixtgrade and ins pired him awsome\")\n",
    "# # max edit distance per lookup (per single word, not per whole input string)\n",
    "# max_edit_distance_lookup = 2\n",
    "# suggestions = sym_spell.lookup_compound(input_term,\n",
    "#                                         max_edit_distance_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "awesome, 1, 15350104\n",
      "awesome\n"
     ]
    }
   ],
   "source": [
    "# lookup suggestions for single-word input strings\n",
    "input_term = \"awsome\"  # misspelling of \"members\"\n",
    "# max edit distance per lookup\n",
    "# (max_edit_distance_lookup <= max_edit_distance_dictionary)\n",
    "max_edit_distance_lookup = 2\n",
    "suggestion_verbosity = Verbosity.CLOSEST  # TOP, CLOSEST, ALL\n",
    "suggestions = sym_spell.lookup(input_term, suggestion_verbosity,\n",
    "                               max_edit_distance_lookup)\n",
    "# display suggestion term, term frequency, and edit distance\n",
    "for suggestion in suggestions:\n",
    "    print(\"{}, {}, {}\".format(suggestion.term, suggestion.distance,\n",
    "                              suggestion.count))\n",
    "print(suggestions[0].term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'part'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "class SpellCorrector():\n",
    "    def __init__(self, max_edit_distance_dictionary=2, prefix_length=7):\n",
    "        self.sym_spell = SymSpell(max_edit_distance_dictionary, prefix_length)\n",
    "        # load dictionary\n",
    "        dictionary_path = os.path.join(os.path.dirname('.'),\n",
    "                                       \"frequency_dictionary_en_82_765.txt\")\n",
    "        term_index = 0  # column of the term in the dictionary text file\n",
    "        count_index = 1  # column of the term frequency in the dictionary text file\n",
    "        if not self.sym_spell.load_dictionary(dictionary_path, term_index, count_index):\n",
    "            raise(\"Dictionary file not found\")\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if len(word) > 2 and \"'\" not in word and not re.search(r'\\d+', word) and not re.search(r'\\w+-\\w+', word):\n",
    "            suggestions = self.sym_spell.lookup(word, Verbosity.CLOSEST, 2)\n",
    "            if suggestions:\n",
    "                return suggestions[0].term\n",
    "        return None\n",
    "    \n",
    "corrector =  SpellCorrector()\n",
    "corrector(\"spart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'so'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrector(\"awsome\")\n",
    "corrector(\"soo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "if re.search(r'\\d+', '10pm'):\n",
    "    print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(r'\\w+-\\w+', '-so')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diggedy digged\n",
      "fuer fuel\n",
      "mit it\n",
      "liebe liege\n",
      "zum cum\n",
      "nicht night\n",
      "nur our\n",
      "nein neil\n",
      "kann ann\n",
      "auch such\n",
      "einen linen\n",
      "leckeren lectern\n",
      "auch such\n",
      "bei be\n",
      "ihm him\n",
      "fuehrt fuehrer\n",
      "everytime overtime\n",
      "flavored flavoured\n",
      "groupon group\n",
      "flavored flavoured\n",
      "namesake- namesake\n",
      "odor door\n",
      "mugginess ugliness\n",
      "awsome awesome\n",
      "bbq bbl\n",
      "alergy allergy\n",
      "rubios rubies\n",
      "ristra vista\n",
      "not.one notion\n",
      "pastries= pastries\n",
      "starswe stars\n",
      "bbq bbl\n",
      ".-menu menu\n",
      "-not not\n",
      "nutella stella\n",
      "center enter\n",
      "rissotto risotto\n",
      "onu on\n",
      "favorite favourite\n",
      "malai malawi\n",
      "kofta soft\n",
      "yelpers helpers\n",
      "potosi photos\n",
      "omg org\n",
      "harbor harbour\n",
      "u.s. us\n",
      "mins mind\n",
      "medioca media\n",
      "serice service\n",
      "barware hardware\n",
      "taste- taste\n",
      "favorite favourite\n",
      "reputaion reputation\n",
      "doesnt does\n",
      "pmg pm\n",
      "pravada pravda\n",
      "pmg pm\n",
      "cowand coward\n",
      "okay- okay\n",
      "off- off\n",
      ".it it\n",
      "centrale central\n",
      "syart start\n",
      "and/or indoor\n",
      "pointe point\n",
      "bnb bob\n",
      "gormet gourmet\n",
      "bbq bbl\n",
      "café cafe\n",
      "ribeye riley\n",
      "whiskey whisky\n",
      "brussel brussels\n",
      "whiskey whisky\n",
      "classico classic\n",
      "zumba rumba\n",
      "dr. dry\n",
      "biehl biel\n",
      "nyny any\n",
      "favorite favourite\n",
      "lennys lenny\n",
      "lennys lenny\n",
      "everytime overtime\n",
      "valle value\n",
      "moco mono\n",
      "wed. wed\n",
      "nite site\n",
      "center enter\n",
      "dont done\n",
      "favorite favourite\n",
      "favorites favourites\n",
      "bbq bbl\n",
      "bleu blue\n",
      "odor door\n",
      "mens men\n",
      "declasse declare\n",
      "tqla tala\n",
      "favorite favourite\n",
      "ayce ace\n",
      "bellagio fellatio\n",
      "mina mind\n",
      "ceviche device\n",
      "von on\n",
      "hansons hanson\n",
      "trumed trumped\n",
      "meds beds\n",
      "meds beds\n",
      "macayo macao\n",
      ".meh meh\n",
      "ajo ago\n",
      "remontant resonant\n",
      "vélo vol\n",
      "suis suit\n",
      "passé pass\n",
      "devant deviant\n",
      "cette bette\n",
      "devanture departure\n",
      "assez asset\n",
      "cocasse cocaine\n",
      "vache cache\n",
      "qui que\n",
      "pète pete\n",
      "suis suit\n",
      "faut fast\n",
      "vraiment raiment\n",
      "sympa spa\n",
      "better.i better\n",
      "favorite favourite\n",
      "theaters heaters\n",
      "biltmore baltimore\n",
      "bbq bbl\n",
      "thier their\n",
      "foood food\n",
      "von on\n",
      "rtd red\n",
      "favor flavor\n",
      "noda node\n",
      "metrodog metrology\n",
      "place.i place\n",
      "brulee rules\n",
      "éviter writer\n",
      "prix pix\n",
      "leur eur\n",
      "vaut vat\n",
      "détour detour\n",
      "mauvais mavis\n",
      "autant mutant\n",
      "goût got\n",
      "leur eur\n",
      "poutine routine\n",
      "pire fire\n",
      "djs dos\n",
      "..that that\n",
      "basic* basic\n",
      "dont done\n",
      ".after after\n",
      "chili child\n",
      "recieving receiving\n",
      "wir air\n",
      "hatten patten\n",
      "für for\n",
      "diesen diesel\n",
      "abend bend\n",
      "stuben steuben\n",
      "ein in\n",
      "platz plate\n",
      "für for\n",
      "personen personal\n",
      "reserviert reservist\n",
      "mal may\n",
      "wieder wider\n",
      "ein in\n",
      "leckeres lectures\n",
      "stück stock\n",
      "das as\n",
      "liegt liege\n",
      "direkt direct\n",
      "bei be\n",
      "verrado serrano\n",
      "bogo logo\n",
      "groupon group\n",
      "color colour\n",
      "maitre mitre\n",
      "as*holes armholes\n",
      "neighborhood neighbourhood\n",
      "matty matt\n",
      "nov. nov\n",
      "nigiri nilgiri\n",
      "opa spa\n",
      "nyc nyx\n",
      "byob bob\n",
      "lunch.. lunch\n",
      "flemings fleming\n",
      "crema.i cremate\n",
      "soo so\n",
      "nite site\n",
      "ribeye riley\n",
      "it.to into\n",
      "fiancé fiance\n",
      "carmax carman\n",
      "definately definitely\n",
      "everytime overtime\n",
      "havent haven\n",
      "baskin basin\n",
      "baskin basin\n",
      "portillos tortillas\n",
      "..it it\n",
      ".this this\n",
      "dipp dip\n",
      "omelet meet\n",
      "ohso ohio\n",
      "beaders readers\n",
      "specialty specially\n",
      "assessory assessor\n",
      "aweful awful\n",
      "bbq bbl\n",
      "sooo soon\n",
      "unbuttered unfettered\n",
      "illy ill\n",
      "pardonne pardon\n",
      "fritte fritter\n",
      "mccrib crib\n",
      "rangoons rangoon\n",
      "tamal tamil\n",
      "snottsdale scottsdale\n",
      "atmosphere.i atmosphere\n",
      "immedietly immediately\n",
      "kitchy itchy\n",
      "jalepeno jalapeno\n",
      "primanti primate\n",
      ".no no\n",
      "traveling travelling\n",
      "dialing dealing\n",
      "vivres gives\n",
      "byow blow\n",
      "tartare tartar\n",
      "bavette babette\n",
      "toooo tools\n",
      "..the the\n",
      "favorite favourite\n",
      "ive live\n",
      "kha khan\n",
      "savory savoy\n",
      "stand.- stand\n",
      "animals.i animals\n",
      "tmoble table\n",
      "boca coca\n",
      "redendo resend\n",
      "drizz frizz\n",
      "frappuccino cappuccino\n",
      "colorful colourful\n",
      "décor decor\n",
      "favorite favourite\n",
      "bbq bbl\n",
      "pasquals pasquale\n",
      "addding adding\n",
      "turnstyle turnstile\n",
      "terries terrier\n",
      "plaza.i plaza\n",
      "ahi chi\n",
      "gilley galley\n",
      "traveling travelling\n",
      "vor for\n",
      "einigen bingen\n",
      "wochen when\n",
      "habe have\n",
      "ich ice\n",
      "diesem diesel\n",
      "einer liner\n",
      "qype type\n",
      "hatte hate\n",
      "hier her\n",
      "abend bend\n",
      "ich ice\n",
      "sagen sage\n",
      "ich ice\n",
      "maßlos males\n",
      "mom mon\n",
      "yelpers helpers\n",
      "mastro castro\n",
      "say.. say\n",
      "seattle.. seattle\n",
      "there.. there\n",
      "average.. average\n",
      "basilthe absinthe\n",
      "favorite favourite\n",
      "..out about\n",
      "..everything everything\n",
      ".especially especially\n",
      "..very very\n",
      ".great great\n",
      "mckoy mccoy\n",
      "lighting- lighting\n",
      "bellagio fellatio\n",
      "kaizen karen\n",
      "clt cut\n",
      "hopple topple\n",
      "favorite favourite\n",
      "wtpho who\n",
      "lot.the loathe\n",
      "savory savoy\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentiment.dev_data:\n",
    "    for t in word_tokenize(sentence):\n",
    "        t = t.lower()\n",
    "        c_t = corrector(t)\n",
    "        if c_t and c_t!=t:\n",
    "            print(t, c_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'awesome'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reduce_lengthening(text):\n",
    "    import re\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)\n",
    "reduce_lengthening(\"aweeeeesome\")\n",
    "reduce_lengthening(\"horrible!!!!!!\")\n",
    "reduce_lengthening(\"awesome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seeeeeeeeeeehr\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "foood\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "f***ing\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "sooo\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "toooo\n",
      "...\n",
      "...\n",
      "...\n",
      "addding\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentiment.dev_data:\n",
    "    for t in word_tokenize(sentence):\n",
    "        t = t.lower()\n",
    "        if reduce_lengthening(t) != t:\n",
    "            print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'__not'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r\"[\\_\\-\\.]+$\", \"\", \"__not__\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n",
      "en\n"
     ]
    }
   ],
   "source": [
    "for ind_ in falseIndex:\n",
    "    print(translator.detect(sentiment.dev_labels[ind_]).lang)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS256",
   "language": "python",
   "name": "cs256"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
